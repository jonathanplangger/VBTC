<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Visual-based Terrain Classification Project: High-resolution networks (HRNets) for Semantic Segmentation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<link href="custom_dark_theme.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Visual-based Terrain Classification Project
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">High-resolution networks (HRNets) for Semantic Segmentation </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="autotoc_md41"></a>
Branches</h1>
<ul>
<li>This is the implementation for HRNet + OCR.</li>
<li>The PyTroch 1.1 version ia available <a href="https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/pytorch-v1.1">here</a>.</li>
<li>The PyTroch 0.4.1 version is available <a href="https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/master">here</a>.</li>
</ul>
<h1><a class="anchor" id="autotoc_md42"></a>
News</h1>
<ul>
<li>[2020/08/16] <a href="https://github.com/open-mmlab/mmsegmentation">MMSegmentation</a> has supported our HRNet + OCR.</li>
<li>[2020/07/20] The researchers from AInnovation have achieved <b>Rank#1</b> on <a href="http://sceneparsing.csail.mit.edu/">ADE20K Leaderboard</a> via training our HRNet + OCR with a semi-supervised learning scheme. More details are in their <a href="https://arxiv.org/pdf/2007.10591.pdf">Technical Report</a>.</li>
<li>[2020/07/09] Our paper is accepted by ECCV 2020: <a href="https://arxiv.org/pdf/1909.11065.pdf">Object-Contextual Representations for Semantic Segmentation</a>. Notably, the reseachers from Nvidia set a new state-of-the-art performance on Cityscapes leaderboard: <a href="https://www.cityscapes-dataset.com/method-details/?submissionID=7836">85.4%</a> via combining our HRNet + OCR with a new <a href="https://arxiv.org/abs/2005.10821">hierarchical mult-scale attention scheme</a>.</li>
<li>[2020/03/13] Our paper is accepted by TPAMI: <a href="https://arxiv.org/pdf/1908.07919.pdf">Deep High-Resolution Representation Learning for Visual Recognition</a>.</li>
<li>HRNet + OCR + SegFix: Rank #1 (84.5) in <a href="https://www.cityscapes-dataset.com/benchmarks/">Cityscapes leaderboard</a>. OCR: object contextual represenations <a href="https://arxiv.org/pdf/1909.11065.pdf">pdf</a>. <em><b>HRNet + OCR is reproduced <a href="https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/HRNet-OCR">here</a></b></em>.</li>
<li>Thanks Google and UIUC researchers. A modified HRNet combined with semantic and instance multi-scale context achieves SOTA panoptic segmentation result on the Mapillary Vista challenge. See <a href="https://arxiv.org/pdf/1910.04751.pdf">the paper</a>.</li>
<li>Small HRNet models for Cityscapes segmentation. Superior to MobileNetV2Plus ....</li>
<li>Rank #1 (83.7) in <a href="https://www.cityscapes-dataset.com/benchmarks/">Cityscapes leaderboard</a>. HRNet combined with an extension of <a href="https://arxiv.org/pdf/1809.00916.pdf">object context</a></li>
<li>Pytorch-v1.1 and the official Sync-BN supported. We have reproduced the cityscapes results on the new codebase. Please check the <a href="https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/pytorch-v1.1">pytorch-v1.1 branch</a>.</li>
</ul>
<h1><a class="anchor" id="autotoc_md43"></a>
Introduction</h1>
<p>This is the official code of <a href="https://arxiv.org/abs/1904.04514">high-resolution representations for Semantic Segmentation</a>. We augment the HRNet with a very simple segmentation head shown in the figure below. We aggregate the output representations at four different resolutions, and then use a 1x1 convolutions to fuse these representations. The output representations is fed into the classifier. We evaluate our methods on three datasets, Cityscapes, PASCAL-Context and LIP.</p>
<p>&lt;figure&gt; &lt;text-align: center;&gt; <img src="./figures/seg-hrnet.png" alt="hrnet" width="900" height="150" class="inline"/> &lt;/figcaption&gt; &lt;/figure&gt;</p>
<p>Besides, we further combine HRNet with <a href="https://arxiv.org/pdf/1909.11065.pdf">Object Contextual Representation</a> and achieve higher performance on the three datasets. The code of HRNet+OCR is contained in this branch. We illustrate the overall framework of OCR in the Figure as shown below:</p>
<p>&lt;figure&gt; &lt;text-align: center;&gt; <img src="./figures/OCR.PNG" alt="OCR" width="900" height="200" class="inline"/> &lt;/figure&gt;</p>
<h1><a class="anchor" id="autotoc_md44"></a>
Segmentation models</h1>
<p>The models are initialized by the weights pretrained on the ImageNet. You can download the pretrained models from <a href="https://github.com/HRNet/HRNet-Image-Classification">https://github.com/HRNet/HRNet-Image-Classification</a>. <em>Slightly different, we use align_corners = True for upsampling in HRNet</em>.</p>
<ol type="1">
<li>Performance on the Cityscapes dataset. The models are trained and tested with the input size of 512x1024 and 1024x2048 respectively. If multi-scale testing is used, we adopt scales: 0.5,0.75,1.0,1.25,1.5,1.75.</li>
</ol>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter">model </th><th class="markdownTableHeadCenter">Train Set </th><th class="markdownTableHeadCenter">Test Set </th><th class="markdownTableHeadCenter">OHEM </th><th class="markdownTableHeadCenter">Multi-scale </th><th class="markdownTableHeadCenter">Flip </th><th class="markdownTableHeadCenter">mIoU </th><th class="markdownTableHeadCenter">Link  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter">HRNetV2-W48 </td><td class="markdownTableBodyCenter">Train </td><td class="markdownTableBodyCenter">Val </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">80.9 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/file/d/15DCds5j95hI-nsjg4eBM1G3sIUWR9tmf/view?usp=sharing">GoogleDrive</a>/<a href="https://pan.baidu.com/s/1KyiOUOR0SYxKtJfIlD5o-w">BaiduYun(Access Code:pmix)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter">HRNetV2-W48 + OCR </td><td class="markdownTableBodyCenter">Train </td><td class="markdownTableBodyCenter">Val </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">81.6 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/file/d/1QDxjWQhkBX_B3qVJykmtYUC3KkXVZIzT/view?usp=sharing">GoogleDrive</a>/<a href="https://pan.baidu.com/s/1BGNt4Xmx3yfXUS8yjde0hQ">BaiduYun(Access Code:fa6i)</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter">HRNetV2-W48 + OCR </td><td class="markdownTableBodyCenter">Train + Val </td><td class="markdownTableBodyCenter">Test </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">82.3 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/file/d/1HiB3pdFhhTtQnrM-zuKrNTmexz_7WmQa/view?usp=sharing">GoogleDrive</a>/<a href="https://pan.baidu.com/s/16mD81UnGzjUBD-haDQfzIQ">BaiduYun(Access Code:ycrk)</a>  </td></tr>
</table>
<ol type="1">
<li>Performance on the LIP dataset. The models are trained and tested with the input size of 473x473.</li>
</ol>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter">model </th><th class="markdownTableHeadCenter">OHEM </th><th class="markdownTableHeadCenter">Multi-scale </th><th class="markdownTableHeadCenter">Flip </th><th class="markdownTableHeadCenter">mIoU </th><th class="markdownTableHeadCenter">Link  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter">HRNetV2-W48 </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">55.83 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/file/d/19Iva2nFGJkvvY9MUs_u3pH7wd50O-L6n/view?usp=sharing">GoogleDrive</a>/<a href="https://pan.baidu.com/s/15DamFiGEoxwDDF1TwuZdnA">BaiduYun(Access Code:fahi)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter">HRNetV2-W48 + OCR </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">56.48 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/file/d/1coUt0IhZ7Ift7Ch7NdeUJAueohsrhEwF/view?usp=sharing">GoogleDrive</a>/<a href="https://pan.baidu.com/s/1dFYSR2bahRnvpIOdh88kOQ">BaiduYun(Access Code:xex2)</a>  </td></tr>
</table>
<p><b>Note</b> Currently we could only reproduce HRNet+OCR results on LIP dataset with PyTorch 0.4.1.</p>
<ol type="1">
<li>Performance on the PASCAL-Context dataset. The models are trained and tested with the input size of 520x520. If multi-scale testing is used, we adopt scales: 0.5,0.75,1.0,1.25,1.5,1.75,2.0 (the same as EncNet, DANet etc.).</li>
</ol>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter">model </th><th class="markdownTableHeadCenter">num classes </th><th class="markdownTableHeadCenter">OHEM </th><th class="markdownTableHeadCenter">Multi-scale </th><th class="markdownTableHeadCenter">Flip </th><th class="markdownTableHeadCenter">mIoU </th><th class="markdownTableHeadCenter">Link  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter">HRNetV2-W48 </td><td class="markdownTableBodyCenter">59 classes </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">54.1 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/file/d/1yUcF4pxO4a2vUAdCUICF-DM2t9KT37hC/view?usp=sharing">GoogleDrive</a>/<a href="https://pan.baidu.com/s/1m0MqpHSk0SX380EYEMawSA">BaiduYun(Access Code:wz6v)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter">HRNetV2-W48 + OCR </td><td class="markdownTableBodyCenter">59 classes </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">56.2 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/file/d/1ubHPoCErl7cYDLjjTHblMeBs1hHWBCOV/view?usp=sharing">GoogleDrive</a>/<a href="https://pan.baidu.com/s/1XYP54gr3XB76tHmCcKdU9g">BaiduYun(Access Code:yyxh)</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter">HRNetV2-W48 </td><td class="markdownTableBodyCenter">60 classes </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">48.3 </td><td class="markdownTableBodyCenter"><a href="https://1drv.ms/u/s!Aus8VCZ_C_33gQEHDQrZCiv4R5mf">OneDrive</a>/<a href="https://pan.baidu.com/s/1pgYt8P8ht2HOOzcA0F7Kag">BaiduYun(Access Code:9uf8)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter">HRNetV2-W48 + OCR </td><td class="markdownTableBodyCenter">60 classes </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">50.1 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/file/d/1ZAZ94GME3wmijF7ax5bqa0P3KxNLPUXR/view?usp=sharing">GoogleDrive</a>/<a href="https://pan.baidu.com/s/13AYjwzh1LJSlipJwNpJ3Uw">BaiduYun(Access Code:gtkb)</a>  </td></tr>
</table>
<ol type="1">
<li>Performance on the COCO-Stuff dataset. The models are trained and tested with the input size of 520x520. If multi-scale testing is used, we adopt scales: 0.5,0.75,1.0,1.25,1.5,1.75,2.0 (the same as EncNet, DANet etc.).</li>
</ol>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter">model </th><th class="markdownTableHeadCenter">OHEM </th><th class="markdownTableHeadCenter">Multi-scale </th><th class="markdownTableHeadCenter">Flip </th><th class="markdownTableHeadCenter">mIoU </th><th class="markdownTableHeadCenter">Link  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter">HRNetV2-W48 </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">36.2 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/open?id=1tXSWTCNyG4ETLfROJM1L6Lswg8wj5WvL">GoogleDrive</a>/<a href="https://pan.baidu.com/s/1VAV6KThH1Irzv9HZgLWE2Q">BaiduYun(Access Code:92gw)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter">HRNetV2-W48 + OCR </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">39.7 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/open?id=1yMJ7-1-7LbbWotrqj1S4vM6M6Nj0feXv">GoogleDrive</a>/<a href="https://pan.baidu.com/s/1HFSYyVwKBG3E6y76gcPjDA">BaiduYun(Access Code:sjc4)</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter">HRNetV2-W48 </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">37.9 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/open?id=1tXSWTCNyG4ETLfROJM1L6Lswg8wj5WvL">GoogleDrive</a>/<a href="https://pan.baidu.com/s/1VAV6KThH1Irzv9HZgLWE2Q">BaiduYun(Access Code:92gw)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter">HRNetV2-W48 + OCR </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">40.6 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/open?id=1yMJ7-1-7LbbWotrqj1S4vM6M6Nj0feXv">GoogleDrive</a>/<a href="https://pan.baidu.com/s/1HFSYyVwKBG3E6y76gcPjDA">BaiduYun(Access Code:sjc4)</a>  </td></tr>
</table>
<p><b>Note</b> We reproduce HRNet+OCR results on COCO-Stuff dataset with PyTorch 0.4.1.</p>
<ol type="1">
<li>Performance on the ADE20K dataset. The models are trained and tested with the input size of 520x520. If multi-scale testing is used, we adopt scales: 0.5,0.75,1.0,1.25,1.5,1.75,2.0 (the same as EncNet, DANet etc.).</li>
</ol>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadCenter">model </th><th class="markdownTableHeadCenter">OHEM </th><th class="markdownTableHeadCenter">Multi-scale </th><th class="markdownTableHeadCenter">Flip </th><th class="markdownTableHeadCenter">mIoU </th><th class="markdownTableHeadCenter">Link  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter">HRNetV2-W48 </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">43.1 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/open?id=1OlTm8k3fIQpZXmOKXipd5BdxVYtbSWt-">GoogleDrive</a>/<a href="https://pan.baidu.com/s/11neVkzxx27qS2-mPFW9dfg">BaiduYun(Access Code:f6xf)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter">HRNetV2-W48 + OCR </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">No </td><td class="markdownTableBodyCenter">44.5 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/open?id=1JEzwhkcPUc-HXnq5ErbWy0vWnNpI9sZ8">GoogleDrive</a>/<a href="https://pan.baidu.com/s/1HLhjiLIdgaOHs0SzEtkgkQ">BaiduYun(Access Code:peg4)</a>  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyCenter">HRNetV2-W48 </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">44.2 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/open?id=1OlTm8k3fIQpZXmOKXipd5BdxVYtbSWt-">GoogleDrive</a>/<a href="https://pan.baidu.com/s/11neVkzxx27qS2-mPFW9dfg">BaiduYun(Access Code:f6xf)</a>  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyCenter">HRNetV2-W48 + OCR </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">Yes </td><td class="markdownTableBodyCenter">45.5 </td><td class="markdownTableBodyCenter"><a href="https://drive.google.com/open?id=1JEzwhkcPUc-HXnq5ErbWy0vWnNpI9sZ8">GoogleDrive</a>/<a href="https://pan.baidu.com/s/1HLhjiLIdgaOHs0SzEtkgkQ">BaiduYun(Access Code:peg4)</a>  </td></tr>
</table>
<p><b>Note</b> We reproduce HRNet+OCR results on ADE20K dataset with PyTorch 0.4.1.</p>
<h1><a class="anchor" id="autotoc_md45"></a>
Quick start</h1>
<h2><a class="anchor" id="autotoc_md46"></a>
Install</h2>
<ol type="1">
<li>For LIP dataset, install PyTorch=0.4.1 following the <a href="https://pytorch.org/">official instructions</a>. For Cityscapes and PASCAL-Context, we use PyTorch=1.1.0.</li>
<li><code>git clone <a href="https://github.com/HRNet/HRNet-Semantic-Segmentation">https://github.com/HRNet/HRNet-Semantic-Segmentation</a> $SEG_ROOT</code></li>
<li>Install dependencies: pip install -r requirements.txt</li>
</ol>
<p>If you want to train and evaluate our models on PASCAL-Context, you need to install <a href="https://github.com/zhanghang1989/detail-api">details</a>. </p><div class="fragment"><div class="line">pip install git+https://github.com/zhanghang1989/detail-api.git#subdirectory=PythonAPI</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md47"></a>
Data preparation</h2>
<p>You need to download the <a href="https://www.cityscapes-dataset.com/">Cityscapes</a>, <a href="http://sysu-hcp.net/lip/">LIP</a> and <a href="https://cs.stanford.edu/~roozbeh/pascal-context/">PASCAL-Context</a> datasets.</p>
<p>Your directory tree should be look like this: </p><div class="fragment"><div class="line">$SEG_ROOT/data</div>
<div class="line">├── cityscapes</div>
<div class="line">│   ├── gtFine</div>
<div class="line">│   │   ├── test</div>
<div class="line">│   │   ├── train</div>
<div class="line">│   │   └── val</div>
<div class="line">│   └── leftImg8bit</div>
<div class="line">│       ├── test</div>
<div class="line">│       ├── train</div>
<div class="line">│       └── val</div>
<div class="line">├── lip</div>
<div class="line">│   ├── TrainVal_images</div>
<div class="line">│   │   ├── train_images</div>
<div class="line">│   │   └── val_images</div>
<div class="line">│   └── TrainVal_parsing_annotations</div>
<div class="line">│       ├── train_segmentations</div>
<div class="line">│       ├── train_segmentations_reversed</div>
<div class="line">│       └── val_segmentations</div>
<div class="line">├── pascal_ctx</div>
<div class="line">│   ├── common</div>
<div class="line">│   ├── PythonAPI</div>
<div class="line">│   ├── res</div>
<div class="line">│   └── VOCdevkit</div>
<div class="line">│       └── VOC2010</div>
<div class="line">├── cocostuff</div>
<div class="line">│   ├── train</div>
<div class="line">│   │   ├── image</div>
<div class="line">│   │   └── label</div>
<div class="line">│   └── val</div>
<div class="line">│       ├── image</div>
<div class="line">│       └── label</div>
<div class="line">├── ade20k</div>
<div class="line">│   ├── train</div>
<div class="line">│   │   ├── image</div>
<div class="line">│   │   └── label</div>
<div class="line">│   └── val</div>
<div class="line">│       ├── image</div>
<div class="line">│       └── label</div>
<div class="line">├── list</div>
<div class="line">│   ├── cityscapes</div>
<div class="line">│   │   ├── test.lst</div>
<div class="line">│   │   ├── trainval.lst</div>
<div class="line">│   │   └── val.lst</div>
<div class="line">│   ├── lip</div>
<div class="line">│   │   ├── testvalList.txt</div>
<div class="line">│   │   ├── trainList.txt</div>
<div class="line">│   │   └── valList.txt</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md48"></a>
Train and Test</h2>
<h3><a class="anchor" id="autotoc_md49"></a>
PyTorch Version Differences</h3>
<p>Note that the codebase supports both PyTorch 0.4.1 and 1.1.0, and they use different command for training. In the following context, we use <code>$PY_CMD</code> to denote different startup command.</p>
<div class="fragment"><div class="line"># For PyTorch 0.4.1</div>
<div class="line">PY_CMD=&quot;python&quot;</div>
<div class="line"># For PyTorch 1.1.0</div>
<div class="line">PY_CMD=&quot;python -m torch.distributed.launch --nproc_per_node=4&quot;</div>
</div><!-- fragment --><p>e.g., when training on Cityscapes, we use PyTorch 1.1.0. So the command </p><div class="fragment"><div class="line">$PY_CMD tools/train.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml</div>
</div><!-- fragment --><p>indicates </p><div class="fragment"><div class="line">python -m torch.distributed.launch --nproc_per_node=4 tools/train.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md50"></a>
Training</h3>
<p>Just specify the configuration file for <code>tools/train.py</code>.</p>
<p>For example, train the HRNet-W48 on Cityscapes with a batch size of 12 on 4 GPUs: </p><div class="fragment"><div class="line">$PY_CMD tools/train.py --cfg experiments/cityscapes/seg_hrnet_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml</div>
</div><!-- fragment --><p>For example, train the HRNet-W48 + OCR on Cityscapes with a batch size of 12 on 4 GPUs: </p><div class="fragment"><div class="line">$PY_CMD tools/train.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml</div>
</div><!-- fragment --><p>Note that we only reproduce HRNet+OCR on LIP dataset using PyTorch 0.4.1. So we recommend to use PyTorch 0.4.1 if you want to train on LIP dataset.</p>
<h3><a class="anchor" id="autotoc_md51"></a>
Testing</h3>
<p>For example, evaluating HRNet+OCR on the Cityscapes validation set with multi-scale and flip testing: </p><div class="fragment"><div class="line">python tools/test.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml \</div>
<div class="line">                     TEST.MODEL_FILE hrnet_ocr_cs_8162_torch11.pth \</div>
<div class="line">                     TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75 \</div>
<div class="line">                     TEST.FLIP_TEST True</div>
</div><!-- fragment --><p>Evaluating HRNet+OCR on the Cityscapes test set with multi-scale and flip testing: </p><div class="fragment"><div class="line">python tools/test.py --cfg experiments/cityscapes/seg_hrnet_ocr_w48_train_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml \</div>
<div class="line">                     DATASET.TEST_SET list/cityscapes/test.lst \</div>
<div class="line">                     TEST.MODEL_FILE hrnet_ocr_trainval_cs_8227_torch11.pth \</div>
<div class="line">                     TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75 \</div>
<div class="line">                     TEST.FLIP_TEST True</div>
</div><!-- fragment --><p>Evaluating HRNet+OCR on the PASCAL-Context validation set with multi-scale and flip testing: </p><div class="fragment"><div class="line">python tools/test.py --cfg experiments/pascal_ctx/seg_hrnet_ocr_w48_cls59_520x520_sgd_lr1e-3_wd1e-4_bs_16_epoch200.yaml \</div>
<div class="line">                     DATASET.TEST_SET testval \</div>
<div class="line">                     TEST.MODEL_FILE hrnet_ocr_pascal_ctx_5618_torch11.pth \</div>
<div class="line">                     TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75,2.0 \</div>
<div class="line">                     TEST.FLIP_TEST True</div>
</div><!-- fragment --><p>Evaluating HRNet+OCR on the LIP validation set with flip testing: </p><div class="fragment"><div class="line">python tools/test.py --cfg experiments/lip/seg_hrnet_w48_473x473_sgd_lr7e-3_wd5e-4_bs_40_epoch150.yaml \</div>
<div class="line">                     DATASET.TEST_SET list/lip/testvalList.txt \</div>
<div class="line">                     TEST.MODEL_FILE hrnet_ocr_lip_5648_torch04.pth \</div>
<div class="line">                     TEST.FLIP_TEST True \</div>
<div class="line">                     TEST.NUM_SAMPLES 0</div>
</div><!-- fragment --><p>Evaluating HRNet+OCR on the COCO-Stuff validation set with multi-scale and flip testing: </p><div class="fragment"><div class="line">python tools/test.py --cfg experiments/cocostuff/seg_hrnet_ocr_w48_520x520_ohem_sgd_lr1e-3_wd1e-4_bs_16_epoch110.yaml \</div>
<div class="line">                     DATASET.TEST_SET list/cocostuff/testval.lst \</div>
<div class="line">                     TEST.MODEL_FILE hrnet_ocr_cocostuff_3965_torch04.pth \</div>
<div class="line">                     TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75,2.0 \</div>
<div class="line">                     TEST.MULTI_SCALE True TEST.FLIP_TEST True</div>
</div><!-- fragment --><p>Evaluating HRNet+OCR on the ADE20K validation set with multi-scale and flip testing: </p><div class="fragment"><div class="line">python tools/test.py --cfg experiments/ade20k/seg_hrnet_ocr_w48_520x520_ohem_sgd_lr2e-2_wd1e-4_bs_16_epoch120.yaml \</div>
<div class="line">                     DATASET.TEST_SET list/cocostuff/testval.lst \</div>
<div class="line">                     TEST.MODEL_FILE hrnet_ocr_ade20k_4451_torch04.pth \</div>
<div class="line">                     TEST.SCALE_LIST 0.5,0.75,1.0,1.25,1.5,1.75,2.0 \</div>
<div class="line">                     TEST.MULTI_SCALE True TEST.FLIP_TEST True</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md52"></a>
Other applications of HRNet</h1>
<ul>
<li><a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch">Human pose estimation</a></li>
<li><a href="https://github.com/HRNet/HRNet-Image-Classification">Image Classification</a></li>
<li><a href="https://github.com/HRNet/HRNet-Object-Detection">Object detection</a></li>
<li><a href="https://github.com/HRNet/HRNet-Facial-Landmark-Detection">Facial landmark detection</a></li>
</ul>
<h1><a class="anchor" id="autotoc_md53"></a>
Citation</h1>
<p>If you find this work or code is helpful in your research, please cite: </p><div class="fragment"><div class="line">@inproceedings{SunXLW19,</div>
<div class="line">  title={Deep High-Resolution Representation Learning for Human Pose Estimation},</div>
<div class="line">  author={Ke Sun and Bin Xiao and Dong Liu and Jingdong Wang},</div>
<div class="line">  booktitle={CVPR},</div>
<div class="line">  year={2019}</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">@article{WangSCJDZLMTWLX19,</div>
<div class="line">  title={Deep High-Resolution Representation Learning for Visual Recognition},</div>
<div class="line">  author={Jingdong Wang and Ke Sun and Tianheng Cheng and </div>
<div class="line">          Borui Jiang and Chaorui Deng and Yang Zhao and Dong Liu and Yadong Mu and </div>
<div class="line">          Mingkui Tan and Xinggang Wang and Wenyu Liu and Bin Xiao},</div>
<div class="line">  journal={TPAMI},</div>
<div class="line">  year={2019}</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">@article{YuanCW19,</div>
<div class="line">  title={Object-Contextual Representations for Semantic Segmentation},</div>
<div class="line">  author={Yuhui Yuan and Xilin Chen and Jingdong Wang},</div>
<div class="line">  booktitle={ECCV},</div>
<div class="line">  year={2020}</div>
<div class="line">}</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md54"></a>
Reference</h1>
<p>[1] Deep High-Resolution Representation Learning for Visual Recognition. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, Bin Xiao. Accepted by TPAMI. <a href="https://arxiv.org/pdf/1908.07919.pdf">download</a></p>
<p>[2] Object-Contextual Representations for Semantic Segmentation. Yuhui Yuan, Xilin Chen, Jingdong Wang. <a href="https://arxiv.org/pdf/1909.11065.pdf">download</a></p>
<h1><a class="anchor" id="autotoc_md55"></a>
Acknowledgement</h1>
<p>We adopt sync-bn implemented by <a href="https://github.com/mapillary/inplace_abn">InplaceABN</a> for PyTorch 0.4.1 experiments and the official sync-bn provided by PyTorch for PyTorch 1.10 experiments.</p>
<p>We adopt data precosessing on the PASCAL-Context dataset, implemented by <a href="https://github.com/zhanghang1989/detail-api">PASCAL API</a>. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
