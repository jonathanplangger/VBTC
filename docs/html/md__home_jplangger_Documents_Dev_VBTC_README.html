<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Visual-based Terrain Classification Project: README</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
<link href="custom_dark_theme.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Visual-based Terrain Classification Project
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">README </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><div align="justify"></div><div align="justify"></div><h1><a class="anchor" id="autotoc_md56"></a>
Vision based Terrain Classification (VBTC) EMSLab Research Repository</h1>
<div align="justify">This repository contains the code for the implementation of Vision-based Terrain Classification Research as part of the Embedded Multi-Sensor Resarch Laboratory at Carleton University (Ottawa, ON). As a research repository, the code relevant to several aspects are included within this repository. Installation instructions for required software solutions and enviroment information is provided in the following sections of the README.MD. We additionally include the implementation instructions for our FCIoU paper below which can be used to recreate the comparative study and experiments detailed in the research manuscript. <br  />
</div><div align="justify"></div><h4><a class="anchor" id="autotoc_md57"></a>
Publications</h4>
<div align="justify">Plangger, Jonathan, Mohamed Atia, and Hicham Chaoui. “FCIoU: A Targeted Approach for Improving Minority Class Detection in Semantic Segmentation Systems.” Machine Learning and Knowledge Extraction 5, no. 4 (November 23, 2023): 1746–59. <a href="https://doi.org/10.3390/make5040085">https://doi.org/10.3390/make5040085</a>.</div><div align="justify"></div><h3><a class="anchor" id="autotoc_md58"></a>
Repository Structure</h3>
<div align="justify">The structure of the repository code is represented as:</div><div align="justify"><img src="github/RepoCodeStructure.png" alt="Structural block diagram of the code contained in the repository" class="inline"/></div><div align="justify">Auto-generated documentation for the project is developed using the Doxygen documentation generator and can be accessed by opening the index.html file in the ./docs/html directory. From there the relevant hyperlinks to the classes and functions within the code repository are present. Note that the documentation is a work in progress and is focused solely on the novel code developed within this repository. As such, classes presented in the models/ directory will not have hand-configured documentation, but are kept for future reference. Only local file hosting of the html documents is currently provided and no plans on web hosting the documentation are currently planned.</div><div align="justify"></div><h3><a class="anchor" id="autotoc_md59"></a>
FCIoU: A Focal Class-based Intersection over Union (IoU) Approach to Improving Minority Class Detection Performance for Off-road Segmentation Systems</h3>
<div align="justify"></div><h2><a class="anchor" id="autotoc_md60"></a>
Abstract</h2>
<div align="justify">In this paper, we present a comparative study of modern semantic segmentation loss functions and their resultant impact when applied with state-of-the-art off-road datasets. Class imbalance, inherent in these datasets, presents a significant challenge to off-road terrain semantic segmentation systems. With numerous environment classes being extremely sparse and underrepresented, model training becomes inefficient and struggles to comprehend the infrequent minority classes. As a solution to this problem, loss functions have been configured to take class imbalance into account and counteract this issue. To this end, we present a novel loss function, Focal Classbased Intersection over Union (FCIoU), which directly targets performance imbalance through the optimization of class-based Intersection over Union (IoU). The new loss function results in a general increase in class-based performance when compared to state-of-the-art targeted loss functions.</div><div align="justify"></div><h3><a class="anchor" id="autotoc_md61"></a>
Getting Started</h3>
<div align="justify"></div><h4><a class="anchor" id="autotoc_md62"></a>
Environment Setup &amp; Installation</h4>
<div align="justify"><b>Python</b> ver. <b>3.8.15</b>, <b>Pytorch</b> ver. <b>1.13.1+cu117</b>, &amp; <b>TorchVision</b> ver. <b>0.14.1+cu117</b> were employed during development. Other, more recent versions of these may operate correctly, however only the specified versions were tested.</div><div align="justify">Other libraries employed are defined within the <em>requirements.txt</em> file included in this repository. Project dependencies can be installed directly through: <div class="fragment"><div class="line">pip install -r requirements.txt</div>
</div><!-- fragment --></div><div align="justify"><a class="anchor" id="autotoc_md63"></a><h5>Running the program</h5>
</div><div align="justify"> Once all the dependencies are installed and resolved, the code can be started by directly running the eval.py and train.py code. The specific configurations by these programs can be configured using the config/config_comparative_study.yaml.</div><div align="justify"></div><h4><a class="anchor" id="autotoc_md64"></a>
First Time Setup</h4>
<div align="justify"> </div><h2><a class="anchor" id="autotoc_md65"></a>
Datasets</h2>
<div align="justify">Before running the program, install one or more of the datasets supported in this development kit. Note that at this moment only the RUGD dataset and Rellis-3D dataset are supported.</div><div align="justify"></div><h3><a class="anchor" id="autotoc_md66"></a>
RUGD Dataset</h3>
<div align="justify">To use the RUGD dataset, the following options must be configured in the <em>config_comparative_study.yaml</em> file:<ul>
<li>DB:<ul>
<li>DB_NAME: "rugd"</li>
<li>PATH: "/path/to/rugd/dataset/"</li>
</ul>
</li>
</ul>
</div><div align="justify">For each of switching, the _C.DB.RUGD.PATH parameter may be configured within the <em>config.py</em> file instead. In doing so, only the DB_NAME parameter in the yaml configuration file has to be altered to switch between datasets.</div><div align="justify">When first using the dataset, pre-conversion of the original data is required to properly utilize the dataset. This is automatically completed by the dataloader and will trigger if a newly obtained dataset missing the conversions is detected. As the conversion is applied to all annotation images within the dataset, the conversion process is expected to take a considerable (~10-25 mins), but will only have to be completed once. The conversion process will result in a new directory "RUGD_modif_frames-with-annotations" which contains re-mapped annotation images. As such, the original directory containing the annotation images "RUGD_annotations" is no longer used by the package and can be safely removed as desired. This pre-conversion was selected to reduce the overhead resulting from the conversion and reduce the overall latency of the dataloader module.</div><div align="justify"></div><h3><a class="anchor" id="autotoc_md67"></a>
Rellis-3D dataset</h3>
<div align="justify">To use the Rellis-3D dataset, the following options must be configured in the <em>config_comparative_study.yaml</em> file:<ul>
<li>DB:<ul>
<li>DB_NAME: "rellis"</li>
<li>PATH: "/path/to/rellis/dataset/"</li>
</ul>
</li>
</ul>
</div><div align="justify">For each of switching, the _C.DB.RUGD.PATH parameter may be configured within the <em>config.py</em> file instead. In doing so, only the DB_NAME parameter in the yaml configuration file has to be altered to switch between datasets.</div><div align="justify"></div><h3><a class="anchor" id="autotoc_md68"></a>
Loss Functions</h3>
<div align="justify">Loss function implementations employed within the study are defined in <em>loss.py</em>. Each loss function were recreated within this study to suit the task of multi-class segmentation. Further information regarding the specific loss functions are provided within the paper manuscript.</div><div align="justify">Selection of loss function for training can be configured through the <em>CRITERION</em> field in the <em>config_comparative_study.yaml</em> file in the <em>configs</em> directory. The possible selections are as follows: <em><ul>
<li>fciouv1 -&gt; fciouv6 (V1 and V2 are Featured within the manuscript)</li>
<li>dicefocal</li>
<li>diceloss</li>
<li>dicetopk</li>
<li>iouloss</li>
<li>powerjaccard</li>
<li>tverskyloss</li>
<li>focalloss </li>
</ul>
</em></div><div align="justify"><em>Selecting a specific loss function in the <em>CRITERION</em> field results in the use of the corresponding loss function class. </em></div><div align="justify"><em></em></div><h3><a class="anchor" id="autotoc_md69"></a>
Models for Comparative Study</h3>
<div align="justify"><em>Contained within the models/ directory is a selection of state-of-the-art semantic segmentation modeles employed in the comparative study. The original implementations of the models were obtained from:</em></div><div align="justify"><em>HRNetv2 + OCR: <a href="https://github.com/HRNet/HRNet-Semantic-Segmentation">https://github.com/HRNet/HRNet-Semantic-Segmentation</a> DeepLabv3+: <a href="https://github.com/VainF/DeepLabV3Plus-Pytorch">https://github.com/VainF/DeepLabV3Plus-Pytorch</a> U-Net: <a href="https://amaarora.github.io/posts/2020-09-13-unet.html">https://amaarora.github.io/posts/2020-09-13-unet.html</a></em></div><div align="justify"><em></em></div><h4><a class="anchor" id="autotoc_md70"></a>
Model Configuration</h4>
<div align="justify"><em>The configuration for the model utilized in training or evaluation can be selected and modified through the <em>MODEL_NAME</em> field in the configuration file.</em></div><div align="justify"><em>The strings to select each of the specified model are as follows:<ul>
<li><em>fciouvX</em>, where <em>X</em> refers to the desired version (fciouv1,etc.)</li>
<li><em>deeplabv3plus</em>, Deeplabv3+</li>
<li><em>hrnet_ocr</em>, HRNetv2 &amp; OCR </li>
</ul>
</em></div></div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.17
</small></address>
</body>
</html>
